<h1>Technical documentation</h1>

<h2>Segmentation.</h2>
To do this project, I am going to use python and OpenCV which is a huge open-source library for the computer vision, machine learning, and image processing that is use in today’s systems to process images and video to identify a large number of objects, part of the body and much more. I need to take some pictures of planes to start, so I used Google Earth and took several pictures of an airport, in this case French Airport "Charles de Gaulle”. The pictures are copyrighted and owned by google. I took enough picture to have a total of 100 airplanes in the total of images to build a dataset. Next step is to create the ground truth. To do so, I copy the images of the airport that we took, and I coloured the airplanes in red, the vegetation in green, the building in blue and in black unknow part. The ground-truth is used to check the result of the machine learning for accuracy and exactitude. It can also be use to supervised the learning of our future model. Supervised learning is when a model use images who are labelled for the training.  In our case we are going to use the supervised learning to teach our network.
After that, I started the image segmentation part which consist to find the regions where the aircrafts are using computer vision. To help me through this task, I look on Internet some tutorials for OpenCV, the lecture on Moodle and try to do the labs. One of the useful labs that helped me a lot is lab 4 which task was to analyse cookies and say that it the cookie was broken, circular or rectangular. The main code to threshold the image inspires me to create my code and understand the steps to take. So, the start of my code is to import sys, numpy and cv2. “cv2” is the OpenCV module which is important to do computer vision tasks. We load the image using cv2.imread and that also permit to work directly on the image later. Next, it is to convert the image which is in RGB (in OpenCV it is not in RGB (Red; Green; Blue) but in BGR (Blue; Green; Red)). We convert it to be apply the threshold on the image and get the shapes of airplanes. To apply the threshold, I have to find the range of colour for the mask that I will be adapted to find the airplane. The aircraft’s colours are white, many shades of grey and some are painted with the colour of their company. We are going to focus on the white and grey colours as they are very similar and will be easier to identify. Aircraft with other colours on it will be ignored to facilitate or task. First, I start by playing around with the value of the range but then I use a website, which is “https://imagecolorpicker.com” to find the colour of the aircraft in RGB. I need to convert their values into BGR. RGB is the format [0-360,0-255,0-255] and HSV is in format [0-180,0-255,0-255]. On the website they use to give the Hue between 0 and 360, Saturation between 0%-100% and Value between 0%-100%. So, we need to convert them to HSV format adapted to OpenCV and python. When I finally found the best mask possible to fin the regions of airplanes, I need to delete small noises that can be around the map and to do so, I use the morphology transformation function of OpenCV. It will do an opening that will do an erosion, which mean that regions are going to shrink and delete the smallest region and then a dilation, which mean that regions will expand and get it normal size. We also do an opening which is a dilation then an erosion that delete small noises inside contours. Some contours of the airplanes are not attached together as it is might be separated by the colours of the fuselage. To attached them together, we do a dilation followed by an erosion so contours can touch and create a region with the whole aircraft inside. The morphological function is applied directly on the mask. Using “cv2.bitwise_and”, we use the main image loaded and we apply the mask on it. So, if we print the result, we will obtain the image with the parts that are in the range of the colour we defined. Now, we need to find and draw the contours of the regions founded. First, we need to convert the image back to BGR using “cv2.cvtColor”. Then, we are going to use cv2.findcontours which will take as parameters the masked image, the mode which is “cv2.RETR_EXTERNAL” that returns that most outer layer of the contour, and the method which is “cv2.CHAIN_APPROX_SIMPLE”, which returns the endpoints to draw the contour. So, with all of that, we should be able to find all the airplanes but there also might be some regions with no airplanes. So, the next step is to discard the maximum of false regions. Let say that the regions that are too small are not airplanes. So, we are going to loop through all the contours and calculate the area of each contour. If the area is superior to the dimension I set, we are drawing a rectangle around the contour so we can see more clearly the region. The rest would be deleted. The contoured regions will be useful because we are going to crop the area inside the rectangle, resize it and keep in a folder to use it for our dataset. 
To use this dataset, we need to label it and separate the aircraft and the non-aircraft images into two separates folders. I did it by hand to split it, but I could do it with a code by comparing the bounding boxes of the original image and the bounding boxes of the ground-truth image. But I was already late on my schedule and I preferred to continue advancing on the project instead of doing it by hand. If I have time at the end of the project, I will try to code it. 

<h2>Classification.</h2>
We now start the next part which is the machine learning part. I am going to choose the R-CNN model for my project because I think it is going to be easiest to use and would be fast to detect object. To help me do this work, I use a tutorial which inspires me to do my project [7]. To implement it, we are going to create three python files: one to put all the config that will be use by the two others file, one file that will set up network for object detection and one file to make everything together to detect object.
In the first file, which we will call it config, we are going to put every configuration needed for the other two files to work. First, we set the input dimension for the neural network. It will only accept images that has this dimension. We also need to set the maximum proposals inference when doing deductions. We also need to define the path to save the model of the aircraft classifier and the label encoder path which will contain the label aircraft and not_aircraft. Then we need to define the minimum probability to filter which prediction is a true positive and which is a false positive. It will delete the region that has the lowest probability to be an aircraft.

We can now do the CNN object detector by using the selective search and classifier together. We are going to use MobileNetV2, which is an architecture of convolutional networks used for object detection. We are creating a new file to create the model. After importing the module that we needed, we are going to do the main part of the code. First, we construct and initialise the initial learning rate, the number of epochs, and the batch size.  EPOCH is the number of times the entire dataset is passed through the algorithm to get trained. The batch size is the number of training examples that will be processed just before it updated the weights in the model.

After the initialization, we are going to load our dataset of aircraft to train our model. We need to create two lists that will receive trained images and trained labels. The two labels are “aircraft” and “no_aircraft”. loop through the directory where the folder of the dataset is, and for each label that we have, we are going to loop through the right folder. For each image inside the folders, the image is read, resized, and then add the image to the trained image list and the label to train labels list. Each image will be linked to a label which will tell if it’s an aircraft or not for the model. We convert the two lists into arrays to provide them to the machine learning algorithm using the NumPy function. On the label array, we are going to apply one-hot encoding to convert the label into a form that the machine can read.

Now, we are going to build the training and testing data by splitting the dataset. 80% of the dataset will be used for the training part and the other 20% is for testing. It is going to split the label and data arrays into 4 subsets. To have a better training set, we are using "ImageDataGenerator" to create more images and add them to the training data. It modifies randomly the angle, the size, the shape and by flipping it to have a better training set and a more efficient system. After this step, we have our training and test data ready to be used.
Next, we load the MobileNetV2 network, pre-trained with ImageNet. A pre-trained model is a model which was trained before with a large dataset. It has the advantage to be a more efficient model and the time to train is reduced. ImageNet is a huge database that contains millions of images and will help us to identify aircraft.

In MobileNetV2, there is a top fully connected layer which is the layer that takes the decision if the object detected is the object wanted, and there is the base where the model will detect the object and pass it to the top fully connected layer. In our case, we do not include the head of the fully connected layer as we are going to build our own custom one. In MobileNetV2, we also need to give the dimension of the network and the number of channels.
 
Next, we construct the new fully connected head with all the parameters needed. The fully connected layers are layers where every neuron of these layers is connected to every neuron of the next layers, so every neuron of every layer is connected to each other. We are building the fully connected head with different parameters such as the number of labels, the number of neurons between each layer, the average pooling and the flatten function. We will add the new head to the model. Also, to not update the layers during the training, we are looping through them and freezing them.

We can now compile the model and use an optimizer to optimise the parameters of the model.
To train our network, we input the parameters of the batch size, epochs, trainX, trainY, testX and testY. We predict on the testing set to see the performance of the model. When the programmes finish running, it saves the model with a format "h5" and saves the label encoder to the disk. When we compile the model, you can see information in real-time about the epoch number, the time it takes, the loss, and the accuracy.


Now for the last step, we are going to make the file that will use our model to analyse images. We first import the necessary package to make our file working.
 
We first construct the argument parser, which will ask the user for an image and then parse the argument. We load our model and the label encoder to use it after. The image is read from the disk and resized.


Then we run a selective search that creates bounding boxes around region proposals. The proposal will be stored in a list as well as their corresponding bounding boxes in another list. We are then looping through the bounding box coordinates of the region proposals, and from that, we extract the region of the bounding box, then convert it to RGB colour, and we resize it with the dimension from the config file. We add the region of interest to the proposal list and the coordinates of the bounding box to the boxes list. The two lists are converted into NumPy arrays to use them for the probability of the model. The model will find the index of all the predictions that are true positives for the aircraft class. The index is used to obtain the bounding boxes and the correspondent class. To eliminate a maximum of true negative results, we are setting a filter that does not accept results that are under 50% probability to be an aircraft. The image is cloned to be able to draw the bounding box around the aircraft founded without modifying the original one. We loop through the bounding box coordinates and draw it on the image as well as the label and probability on the top-left corner of the box. All that is left to do is to display the image with the aircraft found.

<h2>Testing.</h2>
First, we run the machine learning code to teach the model the aircraft images. We have a learning rate of 1e-3, an epoch of 10 and a batch size of 20. We can see for each epoch the time it takes to train, the loss percentage, the accuracy percentage, the value loss percentage, and the value accuracy percentage. We can see that accuracy is high and the time for each epoch is very quick which is due to a small database of aircraft. This information is useful to correct your system and enhance its accuracy. After this, we run detect.py which will detect the aircraft, using the model, on the image that we provided on the command-line. At first, we did not have any recognised planes, so we had to do some changes in the parameters, such as having a bigger epoch for example, to train more the model. I tried to play around with the parameters but did not manage to find a way to find the aircraft on the image. I have a few suppositions about why the model does not find any aircraft. This might be due to the small dataset that I have. In fact, my dataset of aircraft contains only 80 pictures of aircraft. By increasing the number of pictures to a few hundred, it could be a solution to our problem. On the other hand, the model can also be the cause of it. A bad training, a bad dataset or even the wrong parameters in the model can be the problem. So, I should explore these suppositions to find a solution to this obstacle.
